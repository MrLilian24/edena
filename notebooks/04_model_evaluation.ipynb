{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58212f9f",
   "metadata": {},
   "source": [
    "# 04 - Model Evaluation\n",
    "\n",
    "Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82695a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Use GPU if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c902180",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data/raw/soil-classification/Orignal-Dataset\")\n",
    "OUTPUTS_PATH = Path(\"../outputs\")\n",
    "CHECKPOINT_PATH = OUTPUTS_PATH / \"checkpoints\"\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# variables (ImageNet standard)\n",
    "IMG_DEFAULT_SIZE = 256\n",
    "IMG_CROP_SIZE = 224\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 7\n",
    "SEED = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56abdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_DEFAULT_SIZE),\n",
    "    transforms.CenterCrop(IMG_CROP_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = ImageFolder(root=DATA_PATH, transform=test_transform)\n",
    "\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "_, _, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Test set: {len(test_dataset)} images\")\n",
    "print(f\"Classes: {full_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_b0()\n",
    "in_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(in_features, NUM_CLASSES)\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_PATH/'best_model.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Val acc was: {checkpoint['val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f11317",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print(f\"\\nTest Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Correct: {correct}/{total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dce476",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS_PATH / 'confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd15a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = [0] * NUM_CLASSES\n",
    "class_total = [0] * NUM_CLASSES\n",
    "\n",
    "for label, pred in zip(all_labels, all_preds):\n",
    "    class_total[label] += 1\n",
    "    if label == pred:\n",
    "        class_correct[label] += 1\n",
    "\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    acc = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "    print(f\"{name}: {acc:.2f}% ({class_correct[i]}/{class_total[i]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70544131",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "              for i in range(NUM_CLASSES)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(class_names, accuracies)\n",
    "plt.axhline(y=test_acc, color='red', linestyle='--', label=f'Overall: {test_acc:.2f}%')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Per-class Accuracy')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS_PATH / 'per_class_accuracy.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "errors = []\n",
    "for true_label, pred_label in zip(all_labels, all_preds):\n",
    "    if true_label != pred_label:\n",
    "        errors.append((class_names[true_label], class_names[pred_label]))\n",
    "\n",
    "error_counts = Counter(errors)\n",
    "\n",
    "print(f\"\\nTotal errors: {len(errors)}\")\n",
    "print(\"Most common mistakes:\")\n",
    "for (true_class, pred_class), count in error_counts.most_common(5):\n",
    "    print(f\"  {true_class} â†’ {pred_class}: {count}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e39b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSummary:\")\n",
    "print(f\"Test acc: {test_acc:.2f}%\")\n",
    "print(f\"Val acc: {checkpoint['val_acc']:.2f}%\")\n",
    "print(f\"Best class: {class_names[np.argmax(accuracies)]} ({max(accuracies):.2f}%)\")\n",
    "print(f\"Worst: {class_names[np.argmin(accuracies)]} ({min(accuracies):.2f}%)\")  # only 3 samples though\n",
    "print(f\"Errors: {len(errors)}/{len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = {\n",
    "    'test_accuracy': test_acc,\n",
    "    'val_accuracy': checkpoint['val_acc'],\n",
    "    'num_test_samples': len(test_dataset),\n",
    "    'per_class_accuracy': dict(zip(class_names, accuracies)),\n",
    "    'confusion_matrix': cm.tolist()\n",
    "}\n",
    "\n",
    "with open(OUTPUTS_PATH / 'test_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
